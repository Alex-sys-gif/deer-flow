1. Цель продукта
Автоматизация исследовательских задач для менеджеров B2B-продаж
www.loymax.io с помощью ИИ:
Сбор информации о потенциальных клиентах (firmographics)
Поиск контактных данных сотрудников департаментов маркетинга и аналитики
Анализ их активности в соцсетях
Генерация персонализированного контента для первого касания
Работает через веб-интерфейс для отладки и через API для интеграции с Hubspot, Google Workspace и другими системами.

2. Целевая аудитория и сценарии использования
Целевая аудитория:
Менеджеры отдела продаж
Аналитики
Маркетологи
C-level руководители
Сценарии использования:
Быстрое получение актуальной информации о компании клиента
Подготовка персонализированных сообщений с учётом свежих новостей и активности сотрудников
Автоматизация рутинных поисковых задач перед первым контактом

3. Функциональные требования
DeerFlow должен:
Разбивать задачу на шаги (план)
Выполнять поиск в интернете через Tavily/DuckDuckGo/Serper
Анализировать данные и обрабатывать противоречивую информацию
Возвращать результаты в структурированном виде
Интегрироваться с LLM (Groq/Ollama)
Логировать все этапы выполнения
Обеспечивать минимальную настройку и простой деплой

4. Структура шагов (plan steps)
Каждый шаг должен содержать обязательные поля:
title — краткое описание действия (например, "Поиск firmographics")
tool — используемый инструмент (например, TavilySearch)
type — тип действия (search / analyze / generate)
Необязательные поля:
description — подробное описание
sources — ссылки на источники
status — статус выполнения (in progress / success / error)

5. Обработка данных и источников
При отсутствии данных: продолжить выполнение задачи, пометив как "без данных"
При противоречивых данных: использовать комбинированный подход:
Приоритет официального сайта клиента
Голосование по частоте упоминаний
Поддержка нескольких источников:
Общие: Tavily, DuckDuckGo, Serper
Частные: Hubspot, Google Workspace

6. Выходной формат результата (output schema)
Формат: JSON или CSV (в зависимости от потребностей)
Обязательные поля:
company_summary — краткое описание компании (до 3 предложений)
contacts — список найденных контактов с деталями (имя, должность, прошлый опыт, активность за последние 3 месяца)
sources — ссылки на источники
Опциональные поля:
highlights — ключевые моменты для CTA
raw_data — необработанные данные для аудита

7. Логирование и отладка
Все события логируются:
Успех: “Информация о клиенте собрана”
Предупреждение: “Найдены противоречивые данные”
Ошибка: “Не удалось найти контакты”
Логи хранятся в JSON-формате с метаданными: timestamp, тип события, источник.
Через API можно получить историю событий по конкретной задаче.

8. Интеграции и API
DeerFlow поддерживает:
Интеграцию с LLM: Groq, Ollama
Поиск: Tavily, DuckDuckGo, Serper
API для внешних систем: Hubspot, Google Workspace
Веб-интерфейс для отладки (Streamlit)

9. Требования к деплою и CI/CD
Деплой на Render.com (бесплатный тариф)
Автоматический деплой при пуше в main
Поддержка Docker
Минимум API-ключей или использование бесплатных тарифов

10. Ограничения и обработка ошибок
Если инструмент не возвращает результатов — попробовать другой, затем продолжить с пустым результатом
Не блокировать выполнение задачи при отсутствии данных
Проверять наличие обязательных полей (title, tool, type) перед запуском
Ограничивать объем данных для предотвращения ошибок типа 413 Payload Too Large

11. Управление данными и кэширование
Цель
Избежать дублирования поисковых запросов и повторного сбора уже известной информации для повышения производительности и снижения нагрузки на внешние API.

Требования
Кэширование результатов:
Все найденные данные должны кэшироваться с возможностью настройки TTL (Time To Live) — по умолчанию 24 часа
Механизм сравнения данных:
Система должна сравнивать как текстовые совпадения, так и семантическую близость новых запросов к ранее выполненным
Место хранения кэша:
Поддержка нескольких бэкендов: Redis (по умолчанию), файловый кэш, SQLite
Переиспользование данных:
Если найденный факт уже встречался (или похож по смыслу), он должен извлекаться из кэша, а не запрашиваться повторно
Очистка устаревших данных:
Кэш должен автоматически очищаться по истечении TTL или при превышении лимита объема хранения

Режимы работы
MVP : временная память в рамках одной задачи (в RAM), без долгосрочного хранения
Production-ready : использование Redis с настраиваемым TTL и семантическим поиском через embeddings
Интеграции
Поддержка Redis out-of-the-box
Возможность расширения до ChromaDB или FAISS для семантического поиска
Логирование событий кэширования в формате: cache_hit, cache_miss, cache_expired